### Python Libraries and Modules for Data Analysis

This notebook utilizes a variety of Python libraries for data analysis and machine learning. Below is a summary of the imported libraries and their roles:

1. **Data Manipulation and Analysis**
   - `pandas as pd`: Provides data structures and data analysis tools.
   - `numpy as np`: Supports large, multi-dimensional arrays and matrices, along with a collection of mathematical functions.

2. **Statistical Analysis**
   - `scipy.stats`: Contains functions for statistical tests and distribution functions.
     - `ttest_ind`: Performs the independent two-sample t-test.
     - `ttest_rel`: Performs the paired sample t-test.
     - `wilcoxon`: Performs the Wilcoxon signed-rank test.
     - `spearmanr`: Computes Spearman rank-order correlation coefficient.
     - `pearsonr`: Computes Pearson correlation coefficient.
   - `statsmodels.api as sm`: Provides classes and functions for estimating and interpreting statistical models.
   - `statsmodels.formula.api as smf`: Allows for formula-based statistical models.
     - `ols`: Performs ordinary least squares regression.
   - `statsmodels.stats.multitest`: Contains functions for multiple testing correction.
     - `multipletests`: Corrects p-values for multiple comparisons.
   - `statsmodels.stats.multicomp`: Performs pairwise comparisons.
     - `pairwise_tukeyhsd`: Conducts Tukey's HSD test for multiple comparisons.

3. **Data Visualization**
   - `seaborn as sns`: Provides a high-level interface for drawing attractive and informative statistical graphics.
   - `matplotlib.pyplot as plt`: Creates static, animated, and interactive visualizations in Python.

4. **Machine Learning and Model Evaluation**
   - `sklearn.pipeline as Pipeline`: Constructs and evaluates machine learning pipelines.
   - `sklearn.preprocessing`: Scales features for machine learning models.
     - `MinMaxScaler`: Scales features to a given range.
     - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance.
   - `sklearn.linear_model`: Contains linear models for classification and regression.
     - `LogisticRegression`: Logistic regression classifier.
   - `sklearn.svm`: Provides support vector machines for classification and regression.
     - `SVC`: Support Vector Classification.
   - `sklearn.model_selection`: Splits data into training and test sets and performs cross-validation.
   - `LabelEncoder`:   class provided by scikit-learn's preprocessing module used for encoding categorical labels into numerical values.
     - `train_test_split`: Splits arrays or matrices into random train and test subsets.
     - `cross_val_score`: Evaluates a score by cross-validation.
   - `sklearn.metrics`: Evaluates the performance of machine learning models.
     - `classification_report`: Generates a classification report.
     - `roc_curve`: Computes Receiver Operating Characteristic (ROC) curve.
     - `auc`: Computes the Area Under the Curve (AUC) for ROC.

5. **SHAP Values**
   - `shap`: Provides SHAP (SHapley Additive exPlanations) values for model interpretability.
   - `! pip install shap`: Installs the SHAP library if not already available.

These libraries collectively enable comprehensive data analysis, statistical testing, visualization, and machine learning model evaluation.



#Imports

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import ttest_ind
import numpy as np
from scipy.stats import ttest_rel, wilcoxon
from sklearn.preprocessing import MinMaxScaler
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multitest import multipletests
import statsmodels.formula.api as smf
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy.stats import spearmanr, pearsonr
! pip install shap
import shap
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_curve, auc
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV


## Upload all relevant datasets to google drive and mount in notebook to enable comprehensive analysis

from google.colab import drive
drive.mount('/content/drive')

###1. ANALYSIS OF CSER (TIME DOMAIN ENTROPY)

This section demonstrates how to import the CSV file containing CSER results from Google Drive into a pandas DataFrame named `CSER`. The CSV file is located at the specified path in Google Drive.
This section outlines the process of importing, analyzing, and visualizing entropy rate data. The analysis includes calculating descriptive statistics, visualizing the distribution, performing ANOVA, and conducting post hoc tests.

# Import the CSER results CSV file located in the specified Google Drive path into a pandas DataFrame named 'CSER'
CSER = pd.read_csv('/content/drive/MyDrive/CSER DATA/results.csv')
print(CSER)

# Descriptive Statistics

# Calculate mean and standard deviation per Drug and Condition
summary_stats = CSER.groupby(['Drug', 'Condition'])['EntropyRate'].agg(['mean', 'std']).reset_index()
print(summary_stats)

# Visualization
# Plotting the distribution of EntropyRate by Drug and Condition
plt.figure(figsize=(12, 6))
sns.histplot(data=CSER, x='EntropyRate', hue='Condition', multiple='stack', kde=True)
plt.title('Distribution of Entropy Rate by Condition')
plt.xlabel('Entropy Rate')
plt.ylabel('Density')
plt.show()

# Group the data by Participant, Drug, and Condition to calculate the mean EntropyRate for each participant
participant_avg_entropy = CSER.groupby(['Subject', 'Drug', 'Condition'])['EntropyRate'].mean().reset_index()

# Display the result
print(participant_avg_entropy.head())

# Visualization: Box Plot
plt.figure(figsize=(14, 7))
sns.boxplot(data=participant_avg_entropy, x='Drug', y='EntropyRate', hue='Condition', palette='Set1')
plt.title('Distribution of Entropy Rate by Drug and Condition')
plt.xlabel('Drug')
plt.ylabel('Average Entropy Rate')
plt.legend(title='Condition')
plt.show()

#Perform ANOVA and Post Hoc Analysis

# Perform ANOVA using the averaged participant data
model = ols('EntropyRate ~ C(Drug) * C(Condition)', data=participant_avg_entropy).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

# Display the ANOVA table
print(anova_table)


# Create a dataframe with the relevant columns for pairwise comparison
tukey = pairwise_tukeyhsd(endog=participant_avg_entropy['EntropyRate'],
                          groups=participant_avg_entropy['Drug'] + ' ' + participant_avg_entropy['Condition'],
                          alpha=0.05)

# Print the results of Tukey's HSD
print(tukey)


# Convert Tukey's HSD results to a DataFrame for easier plotting
tukey_df = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])

# Plot pairwise comparisons
plt.figure(figsize=(12, 6))
sns.scatterplot(x='meandiff', y='reject', hue='reject', data=tukey_df, palette={True: 'red', False: 'blue'})
plt.axvline(0, linestyle='--', color='grey')
plt.xlabel('Mean Difference')
plt.ylabel('Significance (Reject Null Hypothesis)')
plt.title('Pairwise Comparisons of Entropy Rate')
plt.show()



### 1.1 REGIONAL ANALYSIS OF CSER
This section focuses on analyzing the CSER of different brain regions. The analysis includes defining regions of interest (ROIs), associating channels with these regions, calculating average entropy rates, and visualizing the results.


# Define ROIs and their corresponding regions
roi_regions = {
    'sensorimotor': [1, 2, 17, 18, 57, 58],
    'frontal': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28],
    'temporal': [29, 30, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90],
    'cingulate': [31, 32, 33, 34, 35, 36],
    'limbic': [37, 38, 39, 40, 41, 42],
    'occipital': [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56],
    'parietal': [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70],
    'subcortical': [71, 72, 73, 74, 75, 76, 77, 78]
}

# Reverse the mapping to associate channels with regions
channel_to_roi = {}
for roi, channels in roi_regions.items():
    for channel in channels:
        channel_to_roi[channel] = roi

# Create a new column for ROI
CSER['ROI_Region'] = CSER['Channel'].map(channel_to_roi)

# Display the updated DataFrame
print(CSER)



# Calculate the average entropy rate per subject, region, and condition
regional_subject_avg = CSER.groupby(['Subject', 'ROI_Region', 'Condition'])['EntropyRate'].mean().reset_index()

# Calculate the mean and standard deviation of these regional averages
regional_summary_stats_subject = regional_subject_avg.groupby(['ROI_Region', 'Condition'])['EntropyRate'].agg(['mean', 'std']).reset_index()

print(regional_summary_stats_subject)

#Box plot visualization
plt.figure(figsize=(14, 8))
sns.boxplot(data=regional_subject_avg, x='ROI_Region', y='EntropyRate', hue='Condition')
plt.title('Distribution of Entropy Rates by Brain Region and Condition (Averaged per Subject)')
plt.xlabel('Brain Region')
plt.ylabel('Entropy Rate')
plt.xticks(rotation=45)
plt.legend(title='Condition')
plt.show()


# Create a matrix to hold pairwise entropy comparisons
regions = CSER['ROI_Region'].unique()
pairwise_entropy = pd.DataFrame(index=regions, columns=regions, dtype=float)

for region1 in regions:
    for region2 in regions:
        if region1 != region2:
            # Compare mean entropy rates between two regions
            mean_entropy_region1 = CSER[CSER['ROI_Region'] == region1]['EntropyRate'].mean()
            mean_entropy_region2 = CSER[CSER['ROI_Region'] == region2]['EntropyRate'].mean()
            pairwise_entropy.loc[region1, region2] = np.abs(mean_entropy_region1 - mean_entropy_region2)

# Display the pairwise differences table
print(pairwise_entropy)

# Plot the pairwise entropy differences
plt.figure(figsize=(10, 8))
sns.heatmap(pairwise_entropy, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Pairwise Entropy Differences Between Brain Regions')
plt.xlabel('Region')
plt.ylabel('Region')
plt.show()

### 1.2 TEMPORAL ANALYSIS OF CSER
This section focuses on analyzing the entropy rates over time. Temporal analysis involves examining how the entropy rate evolves during different periods of the experiment. The temporal changes in entropy rates are calculated and visualized to understand any patterns or trends.

# Step 1: Data Preparation
# Define the number of observations per epoch
epoch_size = 1200

# Calculate the number of epochs
num_epochs = len(CSER) // epoch_size

# Generate time points for each epoch
time_points = np.linspace(0, 2, epoch_size)

# Trim the dataset to ensure it's an exact multiple of epoch size
temporal_entropy_perchan = CSER.iloc[:num_epochs * epoch_size].copy()

# Assign time points and epoch indices
temporal_entropy_perchan['Time'] = np.tile(time_points, num_epochs)
temporal_entropy_perchan['Epoch'] = np.repeat(np.arange(num_epochs), epoch_size)
temporal_entropy_perchan['Subject'] = np.repeat(np.arange(len(CSER) // epoch_size), epoch_size)

# Ensure 'EntropyRate' is numeric
temporal_entropy_perchan['EntropyRate'] = pd.to_numeric(temporal_entropy_perchan['EntropyRate'], errors='coerce')

# Calculate average entropy by subject, drug, condition, and epoch
subject_avg_entropy = temporal_entropy_perchan.groupby(['Subject', 'Drug', 'Condition', 'Epoch'])['EntropyRate'].mean().reset_index()

# Check for missing values or infinite values in 'EntropyRate'
print("Missing values in 'EntropyRate':")
print(subject_avg_entropy['EntropyRate'].isna().sum())

# Remove rows with missing or infinite values
subject_avg_entropy = subject_avg_entropy.dropna(subset=['EntropyRate'])


# Step 2: Statistical Analysis

# Check for NaNs or infinite values
print(subject_avg_entropy[['EntropyRate', 'Drug', 'Condition', 'Epoch']].describe())
print(subject_avg_entropy[['EntropyRate', 'Drug', 'Condition', 'Epoch']].isna().sum())

# Ensure 'EntropyRate' is numeric
subject_avg_entropy['EntropyRate'] = pd.to_numeric(subject_avg_entropy['EntropyRate'], errors='coerce')

# Drop rows where 'EntropyRate' is NaN
subject_avg_entropy = subject_avg_entropy.dropna(subset=['EntropyRate'])

# Perform ANOVA
try:
    model = ols('EntropyRate ~ C(Drug) * C(Condition) * C(Epoch)', data=subject_avg_entropy).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)
    print("\nANOVA Table:")
    print(anova_table)
except Exception as e:
    print(f"Error in ANOVA analysis: {e}")


# Perform post-hoc analysis
# Create a combined group variable for Tukey's HSD test
subject_avg_entropy['Group'] = subject_avg_entropy['Drug'] + ' ' + subject_avg_entropy['Condition']
tukey = pairwise_tukeyhsd(endog=subject_avg_entropy['EntropyRate'],
                          groups=subject_avg_entropy['Group'],
                          alpha=0.05)
print("\nTukey's HSD Test Results:")
print(tukey)

# Convert Tukey's HSD results to DataFrame for better readability
tukey_df = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])

# Apply Bonferroni correction
p_values = tukey_df['p-adj'].tolist()
bonferroni_corrected = multipletests(p_values, method='bonferroni')

# Add corrected p-values to DataFrame
tukey_df['Bonferroni Corrected p-value'] = bonferroni_corrected[1]
print("\nBonferroni Corrected p-values:")
print(tukey_df)

# Step 3: Temporal Trends Analysis
# Plot average entropy over time for each drug and condition
plt.figure(figsize=(12, 8))
sns.lineplot(data=subject_avg_entropy, x='Epoch', y='EntropyRate', hue='Drug', style='Condition', ci=None)
plt.xlabel('Time Epoch')
plt.ylabel('Average Entropy Rate')
plt.title('Temporal Changes in Entropy Rate by Drug and Condition')
plt.show()

# Fit a linear model to explore temporal trends
for drug in subject_avg_entropy['Drug'].unique():
    for condition in subject_avg_entropy['Condition'].unique():
        subset = subject_avg_entropy[(subject_avg_entropy['Drug'] == drug) & (subject_avg_entropy['Condition'] == condition)]
        model = ols('EntropyRate ~ Epoch', data=subset).fit()
        print(f"\nDrug: {drug}, Condition: {condition}")
        print(model.summary())

        # Plot results
        plt.figure(figsize=(8, 6))
        sns.scatterplot(x='Epoch', y='EntropyRate', data=subset)
        sns.lineplot(x='Epoch', y=model.fittedvalues, data=subset, color='red')
        plt.xlabel('Time Epoch')
        plt.ylabel('Average Entropy Rate')
        plt.title(f'Entropy Rate vs Time for {drug} under {condition}')
        plt.show()


### 2. SPECTRAL ENTROPY ANALYSIS

This section focuses on analyzing the spectral entropy data across different frequency bands, conditions, and drugs. The goal is to understand how entropy rates vary by frequency band and drug, and to identify any significant differences or patterns.

# Import  and display the spectral CSER results CSV file located in the specified Google Drive path into a pandas DataFrame named 'spectral_entropy_perchan'
spectral_entropy_perchan = pd.read_csv('/content/drive/MyDrive/MEG DATA/spectral_entropy_results_perchan.csv')
print (spectral_entropy_perchan.head())

#Preprocessing
spectral_entropy_perchan = spectral_entropy_perchan.drop(columns = ['ROI'])

# Create a mapping dictionary
band_mapping = {
    'Band_1': 'Delta',
    'Band_2': 'Theta',
    'Band_3': 'Alpha',
    'Band_4': 'Beta',
    'Band_5': 'Gamma'
}

# Replace the frequency band numbers with names
spectral_entropy_perchan['FrequencyBand'] = spectral_entropy_perchan['FrequencyBand'].map(band_mapping)

# Display the updated DataFrame
print(spectral_entropy_perchan)

# Step 1: Data Preparation
# Compute mean entropy for each subject, drug, condition, and frequency band
subject_avg_entropy = spectral_entropy_perchan.groupby(['Subject', 'Drug', 'Condition', 'FrequencyBand'])['EntropyRate'].mean().reset_index()


# Step 2: Descriptive Statistics
# Compute summary statistics
summary_stats = subject_avg_entropy.groupby(['Drug', 'Condition', 'FrequencyBand'])['EntropyRate'].agg(['mean', 'std']).reset_index()
print("Summary Statistics:")
print(summary_stats)

#Visualizing interaction betweenn drug type and freq band
plt.figure(figsize=(12, 8))
sns.pointplot(x='FrequencyBand', y='EntropyRate', hue='Drug', data=subject_avg_entropy, dodge=True, markers=['o', 's', 'D'], linestyles=['-', '--', '-.'])
plt.title('Interaction between Drug and Frequency Band on Entropy Rate')
plt.xlabel('Frequency Band')
plt.ylabel('Entropy Rate')
plt.show()

# Visualization of average entropy by frequency band and condition
frequency_band_avg = subject_avg_entropy.groupby(['FrequencyBand', 'Condition'])['EntropyRate'].mean().reset_index()
frequency_band_pivot = frequency_band_avg.pivot(index='FrequencyBand', columns='Condition', values='EntropyRate')

print(frequency_band_pivot)

plt.figure(figsize=(12, 8))
frequency_band_pivot.plot(kind='bar')
plt.xlabel('Frequency Band')
plt.ylabel('Average Entropy Rate')
plt.title('Average Spectral Entropy by Frequency Band and Condition')
plt.xticks(rotation=45)
plt.legend(title='Condition')
plt.show()


# Step 3: Statistical Analysis
# Perform ANOVA
model = ols('EntropyRate ~ C(Drug) * C(Condition) * C(FrequencyBand)', data=subject_avg_entropy).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print("\nANOVA Table:")
print(anova_table)



# Conduct Tukey's HSD test
subject_avg_entropy['Group'] = subject_avg_entropy['Drug'] + ' ' + subject_avg_entropy['Condition'] + ' ' + subject_avg_entropy['FrequencyBand']
tukey = pairwise_tukeyhsd(endog=subject_avg_entropy['EntropyRate'],
                          groups=subject_avg_entropy['Group'],
                          alpha=0.05)
print("\nTukey's HSD Test Results:")
print(tukey)

# Convert Tukey's HSD results to a DataFrame for plotting
tukey_df = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])

# Apply Bonferroni correction
p_values = tukey_df['p-adj'].tolist()
bonferroni_corrected = multipletests(p_values, method='bonferroni')

# Add corrected p-values to the DataFrame
tukey_df['Bonferroni Corrected p-value'] = bonferroni_corrected[1]
print("\nBonferroni Corrected p-values:")
print(tukey_df)

# Visualization of Tukey's HSD test results
plt.figure(figsize=(12, 6))
sns.scatterplot(x='meandiff', y='reject', hue='reject', data=tukey_df, palette={True: 'red', False: 'blue'})
plt.axvline(0, linestyle='--', color='grey')
plt.xlabel('Mean Difference')
plt.ylabel('Significance (Reject Null Hypothesis)')
plt.title('Pairwise Comparisons of Entropy Rate')
plt.show()

# Step 4: Frequency Band Pairwise Comparisons
# Pairwise comparisons of frequency bands
bands = frequency_band_avg['FrequencyBand'].unique()
pairwise_entropy = pd.DataFrame(index=bands, columns=bands, dtype=float)

for band1 in bands:
    for band2 in bands:
        if band1 != band2:
            mean_entropy_band1 = frequency_band_avg[frequency_band_avg['FrequencyBand'] == band1]['EntropyRate'].mean()
            mean_entropy_band2 = frequency_band_avg[frequency_band_avg['FrequencyBand'] == band2]['EntropyRate'].mean()
            pairwise_entropy.loc[band1, band2] = np.abs(mean_entropy_band1 - mean_entropy_band2)

# Plot the pairwise entropy differences
plt.figure(figsize=(10, 8))
sns.heatmap(pairwise_entropy, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Pairwise Entropy Differences Between Frequency Bands')
plt.xlabel('Frequency Band')
plt.ylabel('Frequency Band')
plt.show()

### 3.Analysis of Lempel-Ziv Complexity (LZ)

This section outlines the analysis of Lempel-Ziv complexity (LZ) results. The analysis includes data import, preprocessing, descriptive statistics, and statistical testing.

# Import and display the LZ results CSV file located in the specified Google Drive path into a pandas DataFrame named 'lempel_ziv_complexity'
lempel_ziv_complexity = pd.read_csv('/content/drive/MyDrive/MEG DATA/lz_complexities.csv')
print(lempel_ziv_complexity.head())

# Descriptive Statistics
# Rename the column header from 'psychedelic' to 'drug'
lempel_ziv_complexity.rename(columns={'Psychedelic': 'Drug'}, inplace=True)

# Rename columns to avoid issues with spaces or special characters
lempel_ziv_complexity.rename(columns={'LZ Complexity': 'LZ'}, inplace=True)


# Display the updated DataFrame to verify the change
print(lempel_ziv_complexity.head())


# Calculate mean and standard deviation per Drug and Condition
summary_stats = lempel_ziv_complexity.groupby(['Drug', 'Condition'])['LZ'].agg(['mean', 'std']).reset_index()
print(summary_stats)

# Visualization
# Plotting the distribution of EntropyRate by Drug and Condition
plt.figure(figsize=(12, 6))
sns.histplot(data=lempel_ziv_complexity, x='LZ', hue='Condition', multiple='stack', kde=True)
plt.title('Distribution of LZ Complexity by Condition')
plt.xlabel('LZ Complexity')
plt.ylabel('Density')
plt.show()


# Group the data by Participant, Drug, and Condition to calculate the mean EntropyRate for each participant
participant_avg_lz = lempel_ziv_complexity.groupby(['Subject', 'Drug', 'Condition'])['LZ'].mean().reset_index()

# Display the result
print(participant_avg_lz.head())

# Visualization: Box Plot
plt.figure(figsize=(14, 7))
sns.boxplot(data=participant_avg_lz, x='Drug', y='LZ', hue='Condition', palette='Set1')
plt.title('Distribution of LZ Complexity by Drug and Condition')
plt.xlabel('Drug')
plt.ylabel('Average LZ Complexity')
plt.legend(title='Condition')
plt.show()

# Perform ANOVA using the averaged participant data
model = ols('LZ ~ C(Drug) * C(Condition)', data=participant_avg_lz).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

# Display the ANOVA table
print(anova_table)


# Create a dataframe with the relevant columns for pairwise comparison
tukey = pairwise_tukeyhsd(endog=participant_avg_lz['LZ'],
                          groups=participant_avg_lz['Drug'] + ' ' + participant_avg_lz['Condition'],
                          alpha=0.05)

# Print the results of Tukey's HSD
print(tukey)


# Convert Tukey's HSD results to a DataFrame for easier plotting
tukey_df = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])

# Plot pairwise comparisons
plt.figure(figsize=(12, 6))
sns.scatterplot(x='meandiff', y='reject', hue='reject', data=tukey_df, palette={True: 'red', False: 'blue'})
plt.axvline(0, linestyle='--', color='grey')
plt.xlabel('Mean Difference')
plt.ylabel('Significance (Reject Null Hypothesis)')
plt.title('Pairwise Comparisons of Entropy Rate')
plt.show()

###3.1 REGIONAL ANALYSIS FOR LZ COMPLEXITY

# Calculate the average entropy rate per subject, region, and condition
regional_subject_avg = lempel_ziv_complexity.groupby(['Subject', 'ROI', 'Condition'])['LZ'].mean().reset_index()

# Calculate the mean and standard deviation of these regional averages
regional_summary_stats_subject = regional_subject_avg.groupby(['ROI', 'Condition'])['LZ'].agg(['mean', 'std']).reset_index()

print(regional_summary_stats_subject)

#Box plot visualization
plt.figure(figsize=(14, 8))
sns.boxplot(data=regional_subject_avg, x='ROI', y='LZ', hue='Condition')
plt.title('Distribution of LZ by Brain Region and Condition (Averaged per Subject)')
plt.xlabel('Brain Region')
plt.ylabel('LZ Complexity')
plt.xticks(rotation=45)
plt.legend(title='Condition')
plt.show()

# Example of a pairwise regional comparison

# Create a matrix to hold pairwise LZ comparisons
regions = lempel_ziv_complexity['ROI'].unique()
pairwise_lz = pd.DataFrame(index=regions, columns=regions, dtype=float)

for region1 in regions:
    for region2 in regions:
        if region1 != region2:
            # Compare mean LZ between two regions
            mean_entropy_region1 = lempel_ziv_complexity[lempel_ziv_complexity['ROI'] == region1]['LZ'].mean()
            mean_entropy_region2 = lempel_ziv_complexity[lempel_ziv_complexity['ROI'] == region2]['LZ'].mean()
            pairwise_lz.loc[region1, region2] = np.abs(mean_entropy_region1 - mean_entropy_region2)

# Display the pairwise differences table
print(pairwise_lz)

# Plot the pairwise LZ differences
plt.figure(figsize=(10, 8))
sns.heatmap(pairwise_lz, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Pairwise LZ Differences Between Brain Regions')
plt.xlabel('Region')
plt.ylabel('Region')
plt.show()

### 4.Comparative Analysis: Lempel-Ziv Complexity (LZ) vs. CSER

This analysis compares Lempel-Ziv Complexity (LZ) and Complex Spectral Entropy Rate (CSER) by integrating their datasets, performing correlation analysis, and visualizing their relationships.

#Ensuring consistency in formatting before merging LZ & CSER dataframes for comparison


# Define a function to perform random sampling
def sample_channels_per_roi(df, roi_col='ROI_Region', channel_col='Channel', num_samples=3):
    # Create an empty DataFrame to store the sampled data
    sampled_df = pd.DataFrame()

    # Get unique ROIs
    rois = df[roi_col].unique()

    for roi in rois:
        # Filter data for the current ROI
        roi_df = df[df[roi_col] == roi]

        # Check if there are enough channels to sample
        if len(roi_df[channel_col].unique()) >= num_samples:
            # Sample channels
            sampled_channels = np.random.choice(roi_df[channel_col].unique(), num_samples, replace=False)

            # Filter data for the sampled channels
            sampled_roi_df = roi_df[roi_df[channel_col].isin(sampled_channels)]

            # Append sampled data to the final DataFrame
            sampled_df = pd.concat([sampled_df, sampled_roi_df])
        else:
            print(f"Not enough channels to sample for ROI {roi}")

    return sampled_df

# Perform random sampling
new_CSER = sample_channels_per_roi(CSER, roi_col='ROI_Region', channel_col='Channel', num_samples=3)

# Display the new CSER subset
print(new_CSER)

#Display LZ dataframe to confirm column consistency
print(lempel_ziv_complexity)

#Restructure condition elements in LZ dataframe to ensure consistency
new_CSER['Condition'] = new_CSER['Condition'].replace({'Drug': 'DRU', 'Placebo': 'PLA'})

# Check columns in both dataframes
print("CSER Columns:", new_CSER.columns)
print("LZC Columns:", lempel_ziv_complexity.columns)

# Ensure both dataframes have the same columns
common_columns = set(new_CSER.columns).intersection(set(lempel_ziv_complexity.columns))
print("Common Columns:", common_columns)

print(new_CSER)
print(lempel_ziv_complexity)

#AGGREGATE & MERGE THE DATASETS

# Aggregating the new_CSER dataset
aggregated_CSER = new_CSER.groupby(['Subject', 'Condition', 'Drug', 'ROI_Region']).agg({'EntropyRate': 'mean'}).reset_index()

# Aggregating the LZC dataset
aggregated_LZ = lempel_ziv_complexity.groupby(['Subject', 'Condition', 'Drug', 'ROI']).agg({'LZ': 'mean'}).reset_index()

# Rename 'ROI_Region' to 'ROI' in the aggregated_CSER for merging
aggregated_CSER.rename(columns={'ROI_Region': 'ROI'}, inplace=True)

# Merge the aggregated datasets on 'Subject', 'Condition', 'Drug', and 'ROI'
merged_data = pd.merge(aggregated_CSER, aggregated_LZ, on=['Subject', 'Condition', 'Drug', 'ROI'])

# Display the merged dataset to verify the result
print(merged_data.head())


#CORRELATION & VISUALIZATION

# Compute the correlation matrix for the merged data
correlation_matrix = merged_data[['EntropyRate', 'LZ']].corr()

# Print the correlation matrix
print("\nCorrelation Matrix between Aggregated EntropyRate and LZ:")
print(correlation_matrix)

# Visualize the relationship between aggregated EntropyRate and LZ

sns.pairplot(merged_data, hue='Condition', vars=['EntropyRate', 'LZ'], palette='Set1')
plt.suptitle('Aggregated Relationship between EntropyRate and LZ under Different Conditions', y=1.02)
plt.show()

### STATISTICAL TESTING OF RELATIONSHIPS

# Spearman's rank correlation
spearman_corr, spearman_p_value = stats.spearmanr(merged_data['EntropyRate'], merged_data['LZ'])
print(f"Spearman's Rank Correlation: coefficient = {spearman_corr}, p-value = {spearman_p_value}")

# Visualizations
plt.figure(figsize=(14, 7))

# 1. Histogram/Density Plot for both EntropyRate and LZ
plt.subplot(1, 3, 1)
sns.kdeplot(merged_data['EntropyRate'], label='Entropy Rate', color='blue')
sns.kdeplot(merged_data['LZ'], label='Lempel-Ziv Complexity', color='orange')
plt.title('Density Plot')
plt.xlabel('Complexity Measure')
plt.legend()

# 2. Scatter Plot
plt.subplot(1, 3, 2)
sns.scatterplot(x=merged_data['EntropyRate'], y=merged_data['LZ'], hue=merged_data['Condition'])
plt.title('Scatter Plot of EntropyRate vs LZ')
plt.xlabel('Entropy Rate')
plt.ylabel('Lempel-Ziv Complexity')
plt.tight_layout()
plt.show()

# Filter data by drug for drug specific analysis
drugs = merged_data['Drug'].unique()

# Create a dictionary to store filtered DataFrames for each drug
drug_dfs = {drug: merged_data[merged_data['Drug'] == drug] for drug in drugs}

# Dictionary to store correlation results
correlation_results = {}

for drug, df in drug_dfs.items():
    # Drop rows with missing values
    df_clean = df.dropna(subset=['EntropyRate', 'LZ'])

    if df_clean.empty:
        correlation_results[drug] = {'correlation': None, 'p-value': None}
        continue

    # Compute Spearman's rank correlation
    corr, p_value = spearmanr(df_clean['EntropyRate'], df_clean['LZ'])
    correlation_results[drug] = {'correlation': corr, 'p-value': p_value}

    print(f"Spearman's Rank Correlation for {drug}: coefficient = {corr:.4f}, p-value = {p_value:.4f}")


# Create a scatter plot for each drug
for drug, df in drug_dfs.items():
    df_clean = df.dropna(subset=['EntropyRate', 'LZ'])

    plt.figure(figsize=(10, 6))
    g = sns.jointplot(data=df_clean, x='EntropyRate', y='LZ', kind='hist', marginal_kws=dict(bins=50, fill=True))
    g.fig.suptitle(f'Marginal Histograms of LZ vs Entropy Rate for {drug}', y=1.02)
    plt.show()

## 5. MACHINE LEARNING MODELS

### 5.1 MODEL TYPE EVALUATION & SELECTION

This section involves preprocessing the data, building machine learning pipelines, and evaluating model performance using Logistic Regression and Support Vector Machine (SVM). The focus is on predicting conditions based on features derived from Entropy Rate and Lempel-Ziv Complexity (LZ).

#Preprocessing & Building out pipelines to test performance of models before selection of main

# Define the preprocessing function
def preprocess_data(data):
    """Preprocess the data by normalizing and scaling."""
    features = ['EntropyRate', 'LZ']
    X = data[features]
    y = data['Condition']

    # Convert categorical labels to binary
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)  # Convert to 0 and 1

    # Initialize and apply the scaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return pd.DataFrame(X_scaled, columns=features), y_encoded


# Preprocess the data
X, y = preprocess_data(merged_data)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define pipelines for Logistic Regression and Support Vector Machine
pipeline_lr = Pipeline([
    ('scaler', StandardScaler()),  # Scaling step
    ('logreg', LogisticRegression(random_state=42))  # Logistic Regression step
])

pipeline_svm = Pipeline([
    ('scaler', StandardScaler()),  # Scaling step
    ('svm', SVC(probability=True, random_state=42))  # SVM step
])

# Train models
pipeline_lr.fit(X_train, y_train)
pipeline_svm.fit(X_train, y_train)

# Evaluate Logistic Regression
y_pred_lr = pipeline_lr.predict(X_test)
print("Logistic Regression Classification Report:")
print(classification_report(y_test, y_pred_lr))

# ROC Curve for Logistic Regression
y_prob_lr = pipeline_lr.predict_proba(X_test)[:, 1]
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)
roc_auc_lr = auc(fpr_lr, tpr_lr)

plt.figure()
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (area = {roc_auc_lr:.2f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc='lower right')
plt.show()

# Evaluate SVM
y_pred_svm = pipeline_svm.predict(X_test)
print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm))

# ROC Curve for SVM
y_prob_svm = pipeline_svm.predict_proba(X_test)[:, 1]
fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm)
roc_auc_svm = auc(fpr_svm, tpr_svm)

plt.figure()
plt.plot(fpr_svm, tpr_svm, label=f'SVM (area = {roc_auc_svm:.2f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - SVM')
plt.legend(loc='lower right')
plt.show()

### 5.3  Implementation of selected (most efficient) model: Logistic Regression.


# Define the full pipeline
def create_pipeline():
    # Initialize the model
    log_reg = LogisticRegression(max_iter=1000)

    # Create the pipeline
    pipeline = Pipeline([
        ('scaler', StandardScaler()),   # Scaling
        ('log_reg', log_reg)            # Model
    ])

    return pipeline

# Define the hyperparameters to tune
param_grid = {
    'log_reg__C': [0.001, 0.01, 0.1, 1, 10, 100],
    'log_reg__solver': ['liblinear', 'saga'],  # 'liblinear' for smaller datasets and 'saga' for larger ones
    'log_reg__penalty': ['l1', 'l2']
}

# Preprocess the data
def preprocess_data(data):
    """Preprocess the data by normalizing and scaling."""
    features = ['EntropyRate', 'LZ']
    X = data[features]
    y = data['Condition']

    # Encode the target variable
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    return X, y_encoded, le

# Load and preprocess the data
X, y, le = preprocess_data(merged_data)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Create the pipeline
pipeline = create_pipeline()

# Perform Grid Search with Cross-Validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Print the best parameters
print(f"Best Parameters: {grid_search.best_params_}")

# Train the model with the best parameters
best_pipeline = grid_search.best_estimator_

# Predict on the test set
y_pred = best_pipeline.predict(X_test)
y_prob = best_pipeline.predict_proba(X_test)[:, 1]

# Evaluation
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# Compute ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Cross-Validation Scores
cv_scores = cross_val_score(best_pipeline, X, y, cv=5)
print(f"Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean Cross-Validation Accuracy: {cv_scores.mean()}")



# SHAP Values
# Create a SHAP explainer
explainer = shap.Explainer(best_pipeline.named_steps['log_reg'], X_train)

# Compute SHAP values
shap_values = explainer(X_test)

# Plot SHAP summary
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, feature_names=['EntropyRate', 'LZ'])


# Assuming shap_values is already computed from the SHAP explainer
shap_values = explainer(X_test)

###5.3 Building Participant-Level Logistic Regression Models for Each Drug

In this section, we will create logistic regression models for each drug type to predict the condition based on features derived from Entropy Rate and Lempel-Ziv Complexity (LZ). The goal is to analyze how each drug affects the predictive model.


# Define the preprocessing and model pipeline
def create_logistic_regression_pipeline():
    """Create a pipeline for Logistic Regression."""
    pipeline = Pipeline([
        ('scaler', StandardScaler()),  # Feature scaling
        ('clf', LogisticRegression())  # Logistic Regression classifier
    ])

    # Define hyperparameters for Grid Search
    param_grid = {
        'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],
        'clf__solver': ['liblinear', 'saga'],  # 'liblinear' for smaller datasets and 'saga' for larger ones
        'clf__penalty': ['l1', 'l2']
    }

    # Perform Grid Search with Cross-Validation
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

    return grid_search

def preprocess_data(data):
    """Preprocess the data by normalizing and scaling."""
    features = ['EntropyRate', 'LZ']
    X = data[features]
    y = data['Condition']

    # Encode the target variable
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    return X, y_encoded, le

def evaluate_model(best_model, X_test, y_test, le):
    """Evaluate the model using classification report and ROC curve."""
    y_pred = best_model.predict(X_test)
    y_prob = best_model.predict_proba(X_test)[:, 1]

    # Classification Report
    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(10, 6))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()

def analyze_shap(best_model, X_train, X_test):
    """Analyze SHAP values for feature importance."""
    # Extract the Logistic Regression model from the pipeline
    model = best_model.named_steps['clf']

    # Initialize SHAP explainer with the model
    explainer = shap.LinearExplainer(model, X_train)

    # Calculate SHAP values
    shap_values = explainer.shap_values(X_test)

    # Summary Plot
    shap.summary_plot(shap_values, X_test, feature_names=['EntropyRate', 'LZ'])

# Assuming merged_data is your DataFrame
drug_groups = merged_data['Drug'].unique()

# Split the data by drug group
drug_data = {drug: merged_data[merged_data['Drug'] == drug] for drug in drug_groups}

# Iterate over each drug group
for drug, data in drug_data.items():
    participants = data['Subject'].unique()

    for participant in participants:
        # Filter data for this participant
        participant_data = data[data['Subject'] == participant]

        # Preprocess data
        X, y, le = preprocess_data(participant_data)

        # Split data into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

        # Create and train model
        grid_search = create_logistic_regression_pipeline()
        grid_search.fit(X_train, y_train)

        # Print the best parameters
        print(f"Drug: {drug}, Participant: {participant}")
        print(f"Best Parameters: {grid_search.best_params_}")

        # Train and evaluate the best model
        best_model = grid_search.best_estimator_
        evaluate_model(best_model, X_test, y_test, le)

        # Analyze SHAP values
        analyze_shap(best_model, X_train, X_test)


